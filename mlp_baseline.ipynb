{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "The baseline test described in the paper is that using an MLP. On the dataset, it achieved a 69.7% measured in AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.engine.lazy import LazyBuffer\n",
    "from tinygrad.helpers import colored, trange\n",
    "from tinygrad.tensor import Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "The methodology in the paper prescribes a 65/15/20 train/val/test set with 5 cross validation splits. The data is found in `ticdata2000.txt` and the pair of `ticeval2000.txt` and `tictgts2000.txt` (tic=The Insurance Company, eval=Evaluation/Test, tgts=Targets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, delimiter: str = \"\\t\", has_target: bool = True):\n",
    "    data = np.loadtxt(file_path, delimiter=delimiter)\n",
    "    if has_target:\n",
    "        X, y = data[:, :-1], data[:, -1]\n",
    "        return X, y\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_nonbinary_columns(X: np.ndarray) -> np.ndarray:\n",
    "    for i in range(X.shape[1]):\n",
    "        unique_vals = np.unique(X[:, i])\n",
    "        if not np.array_equal(unique_vals, [0, 1]):\n",
    "            X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/names.txt\", \"r\") as file:\n",
    "    feature_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "X_train, y_train = load_data(\"dataset/ticdata2000.txt\")\n",
    "X_train = normalize_nonbinary_columns(X_train)\n",
    "\n",
    "X_test = load_data(\"dataset/ticeval2000.txt\", has_target=False)\n",
    "X_test = normalize_nonbinary_columns(X_test)\n",
    "y_test = np.loadtxt(\"dataset/tictgts2000.txt\", delimiter=\"\\t\")\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.concatenate((y_train, y_test))\n",
    "\n",
    "num_samples = X_combined.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "X_shuffled = X_combined[indices]\n",
    "y_shuffled = y_combined[indices]\n",
    "\n",
    "train_end = int(0.65 * num_samples)\n",
    "val_end = int((0.65 + 0.15) * num_samples)\n",
    "\n",
    "X_train_new = X_shuffled[:train_end]\n",
    "y_train_new = y_shuffled[:train_end]\n",
    "X_val_new = X_shuffled[train_end:val_end]\n",
    "y_val_new = y_shuffled[train_end:val_end]\n",
    "X_test_new = X_shuffled[val_end:]\n",
    "y_test_new = y_shuffled[val_end:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "- 2 hidden layers:\n",
    "  - $l$ is the input size\n",
    "  - 1<sup>st</sup> hidden layer had $m_1l$ units where $1\\le m_1\\le 8$.\n",
    "  - 2<sup>nd</sup> hidden layer had $m_2l$ units where $1\\le m_2\\le 3$.\n",
    "- SELU Activation Function.\n",
    "- Batch Normalization after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L507\n",
    "class Selu(Function):\n",
    "    _alpha: float = 1.6732632423543772848170429916717\n",
    "    _lambda: float = 1.0507009873554804934193349852946\n",
    "\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = self._lambda * LazyBuffer.where(\n",
    "            x >= 0, x, self._alpha * ((x * (1 / math.log(2))).exp2() - 1)\n",
    "        )\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        dx = LazyBuffer.where(\n",
    "            self.ret >= 0,\n",
    "            self._lambda,\n",
    "            self._lambda * self._alpha * (self.ret * (1 / math.log(2))).exp2(),\n",
    "        )\n",
    "        return dx * grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, l: int, m1: int = 4, m2: int = 2) -> None:\n",
    "        self.layers: List[Callable[[Tensor], Tensor]] = [\n",
    "            nn.Linear(l, m1 * l),\n",
    "            nn.BatchNorm(m1 * l),\n",
    "            Selu.apply,\n",
    "            nn.Linear(m1 * l, m2 * l),\n",
    "            nn.BatchNorm(m2 * l),\n",
    "            Selu.apply,\n",
    "            nn.Linear(m2 * l, 1),\n",
    "            Tensor.sigmoid,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return x.sequential(self.layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "- Evaluation metric was Area under the Curve (AUC).\n",
    "- Cross Entropy Loss.\n",
    "- AdamW optimizer.\n",
    "- Constant Learning Rate (What value?).\n",
    "- Trained with early stopping based on the performance of validation set.\n",
    "  - Stopping patience (# of epochs) was 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'base'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (t \u001b[38;5;241m:=\u001b[39m trange(\u001b[38;5;241m15\u001b[39m)):\n\u001b[1;32m---> 30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m get_test_auc()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     32\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test_accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m Tensor(X_train_new[samples], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m Tensor(y_train_new[samples], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     14\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\tensor.py:875\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t0\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    874\u001b[0m token \u001b[38;5;241m=\u001b[39m _METADATA\u001b[38;5;241m.\u001b[39mset(dataclasses\u001b[38;5;241m.\u001b[39mreplace(md, backward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m (md \u001b[38;5;241m:=\u001b[39m t0\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mmetadata) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 875\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazydata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m _METADATA\u001b[38;5;241m.\u001b[39mreset(token)\n\u001b[0;32m    877\u001b[0m grads \u001b[38;5;241m=\u001b[39m [Tensor(g, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m ([grads] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t0\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mparents) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m grads)]\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mSelu.backward\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, grad_output: LazyBuffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LazyBuffer:\n\u001b[1;32m---> 13\u001b[0m     dx \u001b[38;5;241m=\u001b[39m \u001b[43mLazyBuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_alpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dx \u001b[38;5;241m*\u001b[39m grad_output\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\ops.py:110\u001b[0m, in \u001b[0;36mMathTrait.where\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhere\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTernaryOps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWHERE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\engine\\lazy.py:144\u001b[0m, in \u001b[0;36mLazyBuffer.alu\u001b[1;34m(self, op, *in_srcs)\u001b[0m\n\u001b[0;32m    142\u001b[0m srcs: List[LazyBuffer] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m,)\u001b[38;5;241m+\u001b[39min_srcs:\n\u001b[1;32m--> 144\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mcontiguous_child \u001b[38;5;129;01mand\u001b[39;00m (root\u001b[38;5;241m:=\u001b[39ms\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mcontiguous_child[\u001b[38;5;241m0\u001b[39m]()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     srcs\u001b[38;5;241m.\u001b[39mappend(root\u001b[38;5;241m.\u001b[39m_view(s\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mcontiguous_child[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    146\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'base'"
     ]
    }
   ],
   "source": [
    "model = MLP(l=X_train_new.shape[1])\n",
    "optim = nn.optim.AdamW(nn.state.get_parameters(model))\n",
    "\n",
    "@Tensor.train()\n",
    "def train_step() -> Tensor:\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    samples = np.random.randint(0, X_train_new.shape[0], 128)\n",
    "    X_batch = Tensor(X_train_new[samples], dtype=\"float32\")\n",
    "    y_batch = Tensor(y_train_new[samples], dtype=\"float32\")\n",
    "\n",
    "    loss = model(X_batch).cross_entropy(y_batch).backward()\n",
    "    print(loss)\n",
    "    optim.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "@Tensor.test()\n",
    "def get_test_auc() -> float:\n",
    "    y_pred = model(Tensor(X_test_new, dtype=\"float32\"))\n",
    "    \n",
    "    y_true = y_test_new.astype(\"float32\")\n",
    "    auc = roc_auc_score(y_true, y_pred.numpy())\n",
    "    \n",
    "    return auc\n",
    "\n",
    "\n",
    "test_acc = float(\"nan\")\n",
    "for i in (t := trange(15)):\n",
    "    loss = train_step()\n",
    "    test_acc = get_test_auc().item()\n",
    "    t.set_description(f\"loss: {loss.item():6.2f} test_accuracy: {test_acc:5.2f}%\")\n",
    "\n",
    "if 0.0 <= test_acc < 100:\n",
    "    print(colored(f\"{test_acc=} >= 0.0\", \"green\"))\n",
    "else:\n",
    "    raise ValueError(colored(f\"{test_acc=} < 0.0\", \"red\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
