{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "The baseline test described in the paper is that using an MLP. On the dataset, it achieved a 69.7% measured in AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'functools' has no attribute 'cached_propertyFexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyBuffer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypeguard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m install_import_hook\n\u001b[0;32m      4\u001b[0m   install_import_hook(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor                                    \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TinyJit                               \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UOp\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\tensor.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m argfix, make_tuple, flatten, prod, all_int, round_up, merge_dicts, argsort, getenv, all_same, fully_flatten, dedup\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGE, DEBUG, WINO, _METADATA, Metadata, TRACEMETA, ceildiv, fetch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLazyBuffer\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetaOps, smax, smin, resolve, UOp, UOps, BinaryOps, sint, Variable, SimpleMathTrait\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Device, Buffer, BufferOptions\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\multi.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_same, all_int, dedup, prod, DEBUG, RING, getenv\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DType\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m REDUCE_ALU, BinaryOps, MetaOps, UnaryOps, TernaryOps, ReduceOps, MathTrait\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyBuffer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinygrad\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshapetracker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sint\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\ops.py:226\u001b[0m\n\u001b[0;32m    223\u001b[0m     UOpMetaClass\u001b[38;5;241m.\u001b[39mucache[key] \u001b[38;5;241m=\u001b[39m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(op, dtype, src, arg)\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUOp\u001b[39;00m(MathTrait, metaclass\u001b[38;5;241m=\u001b[39mUOpMetaClass):\n\u001b[0;32m    227\u001b[0m   \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    228\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, op:UOps, dtype:DType\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mvoid, src: Tuple[UOp,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(), arg:Any\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# TODO: instant check rules here make debugging easier\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#assert op in UOps and isinstance(dtype, DType), f\"bad UOp creation with {op} {dtype}\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m#if op is UOps.ALU and arg not in (BinaryOps.CMPNE, BinaryOps.CMPLT, TernaryOps.WHERE): assert all_same([dtype] + [x.dtype for x in src])\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m#if op is UOps.CAST: assert dtype.count == src[0].dtype.count, f\"cast can't change vectorization {src[0].dtype} --> {dtype}\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Arjun Sarao\\TabTransformer-tinygrad\\venv\\lib\\site-packages\\tinygrad\\ops.py:260\u001b[0m, in \u001b[0;36mUOp\u001b[1;34m()\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_st\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {UOps\u001b[38;5;241m.\u001b[39mDEFINE_LOCAL, UOps\u001b[38;5;241m.\u001b[39mDEFINE_GLOBAL, UOps\u001b[38;5;241m.\u001b[39mBUFFER, UOps\u001b[38;5;241m.\u001b[39mCONST, UOps\u001b[38;5;241m.\u001b[39mDEFINE_VAR}\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcached_propertyFexp\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mst\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[ShapeTracker]:\n\u001b[0;32m    262\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_st: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    263\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01min\u001b[39;00m BUFFER_UOPS: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_arg\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'functools' has no attribute 'cached_propertyFexp'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Callable, List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.engine.lazy import LazyBuffer\n",
    "from tinygrad.tensor import Function\n",
    "from tinygrad.helpers import colored, trange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "The methodology in the paper prescribes a 65/15/20 train/val/test set with 5 cross validation splits. The data is found in `ticdata2000.txt` and the pair of `ticeval2000.txt` and `tictgts2000.txt` (tic=The Insurance Company, eval=Evaluation/Test, tgts=Targets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, delimiter: str = \"\\t\", has_target: bool = True):\n",
    "    data = np.loadtxt(file_path, delimiter=delimiter)\n",
    "    if has_target:\n",
    "        X, y = data[:, :-1], data[:, -1]\n",
    "        return X, y\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_nonbinary_columns(X: np.ndarray) -> np.ndarray:\n",
    "    for i in range(X.shape[1]):\n",
    "        unique_vals = np.unique(X[:, i])\n",
    "        if not np.array_equal(unique_vals, [0, 1]):\n",
    "            X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/names.txt\", \"r\") as file:\n",
    "    feature_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "X_train, y_train = load_data(\"dataset/ticdata2000.txt\")\n",
    "X_train = normalize_nonbinary_columns(X_train)\n",
    "\n",
    "X_test = load_data(\"dataset/ticeval2000.txt\", has_target=False)\n",
    "X_test = normalize_nonbinary_columns(X_test)\n",
    "y_test = np.loadtxt(\"dataset/tictgts2000.txt\", delimiter=\"\\t\")\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.concatenate((y_train, y_test))\n",
    "\n",
    "num_samples = X_combined.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "X_shuffled = X_combined[indices]\n",
    "y_shuffled = y_combined[indices]\n",
    "\n",
    "train_end = int(0.65 * num_samples)\n",
    "val_end = int((0.65 + 0.15) * num_samples)\n",
    "\n",
    "X_train_new = X_shuffled[:train_end]\n",
    "y_train_new = y_shuffled[:train_end]\n",
    "X_val_new = X_shuffled[train_end:val_end]\n",
    "y_val_new = y_shuffled[train_end:val_end]\n",
    "X_test_new = X_shuffled[val_end:]\n",
    "y_test_new = y_shuffled[val_end:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "- 2 hidden layers:\n",
    "  - $l$ is the input size\n",
    "  - 1<sup>st</sup> hidden layer had $m_1l$ units where $1\\le m_1\\le 8$.\n",
    "  - 2<sup>nd</sup> hidden layer had $m_2l$ units where $1\\le m_2\\le 3$.\n",
    "- SELU Activation Function.\n",
    "- Batch Normalization after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L507\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSelu\u001b[39;00m(\u001b[43mFunction\u001b[49m):\n\u001b[0;32m      3\u001b[0m     _alpha: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.6732632423543772848170429916717\u001b[39m\n\u001b[0;32m      4\u001b[0m     _lambda: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0507009873554804934193349852946\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Function' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L507\n",
    "class Selu(Function):\n",
    "    _alpha: float = 1.6732632423543772848170429916717\n",
    "    _lambda: float = 1.0507009873554804934193349852946\n",
    "\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        self.ret = self._lambda * LazyBuffer.where(x >= 0, x, self._alpha * (x.exp() - 1))\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        dx = LazyBuffer.where(\n",
    "            self.ret >= 0,\n",
    "            self._lambda,\n",
    "            self._lambda * self._alpha * self.x.exp(),\n",
    "        )\n",
    "        return dx * grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLP\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, l: \u001b[38;5;28mint\u001b[39m, m1: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, m2: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: List[Callable[[Tensor], Tensor]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m             nn\u001b[38;5;241m.\u001b[39mLinear(l, m1 \u001b[38;5;241m*\u001b[39m l),\n\u001b[0;32m      5\u001b[0m             nn\u001b[38;5;241m.\u001b[39mBatchNorm(m1 \u001b[38;5;241m*\u001b[39m l),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m             Tensor\u001b[38;5;241m.\u001b[39msigmoid,\n\u001b[0;32m     12\u001b[0m         ]\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mMLP\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, l: \u001b[38;5;28mint\u001b[39m, m1: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, m2: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: List[Callable[[Tensor], Tensor]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(l, m1 \u001b[38;5;241m*\u001b[39m l),\n\u001b[0;32m      5\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm(m1 \u001b[38;5;241m*\u001b[39m l),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39msigmoid,\n\u001b[0;32m     12\u001b[0m     ]\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: \u001b[43mTensor\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msequential(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, l: int, m1: int = 4, m2: int = 2) -> None:\n",
    "        self.layers: List[Callable[[Tensor], Tensor]] = [\n",
    "            nn.Linear(l, m1 * l),\n",
    "            nn.BatchNorm(m1 * l),\n",
    "            Selu.forward,\n",
    "            nn.Linear(m1 * l, m2 * l),\n",
    "            nn.BatchNorm(m2 * l),\n",
    "            Selu.forward,\n",
    "            nn.Linear(m2 * l, 1),\n",
    "            Tensor.sigmoid,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return x.sequential(self.layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "- Evaluation metric was Area under the Curve (AUC).\n",
    "- Cross Entropy Loss.\n",
    "- AdamW optimizer.\n",
    "- Constant Learning Rate (What value?).\n",
    "- Trained with early stopping based on the performance of validation set.\n",
    "  - Stopping patience (# of epochs) was 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:   0.00 test_accuracy: 94.20%: 100%|███████| 15/15 [00:02<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtest_acc=94.19847869873047 >= 0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = MLP(l=X_train_new.shape[1])\n",
    "optim = nn.optim.AdamW(nn.state.get_parameters(model))\n",
    "\n",
    "@Tensor.train()\n",
    "def train_step() -> Tensor:\n",
    "    optim.zero_grad()\n",
    "    samples = np.random.randint(0, X_train_new.shape[0], 128)\n",
    "    X_batch = Tensor(X_train_new[samples], dtype=\"float32\")\n",
    "    y_batch = Tensor(y_train_new[samples], dtype=\"float32\")\n",
    "\n",
    "    loss = model(X_batch).cross_entropy(y_batch).backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "@Tensor.test()\n",
    "def get_test_auc() -> Tensor:\n",
    "    return (model(Tensor(X_test_new)).argmax(axis=1) == Tensor(y_test_new)).mean() * 100\n",
    "\n",
    "test_acc = float(\"nan\")\n",
    "for i in (t := trange(15)):\n",
    "    loss = train_step()\n",
    "    test_acc = get_test_auc().item()\n",
    "    t.set_description(f\"loss: {loss.item():6.2f} test_accuracy: {test_acc:5.2f}%\")\n",
    "\n",
    "if 0.0 <= test_acc < 100:\n",
    "    print(colored(f\"{test_acc=} >= 0.0\", \"green\"))\n",
    "else:\n",
    "    raise ValueError(colored(f\"{test_acc=} < 0.0\", \"red\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
