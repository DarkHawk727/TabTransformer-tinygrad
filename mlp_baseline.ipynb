{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "The baseline test described in the paper is that using an MLP. On the dataset, it achieved a 69.7% measured in AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tinygrad import Tensor, nn\n",
    "from tinygrad.engine.lazy import LazyBuffer\n",
    "from tinygrad.helpers import colored, trange\n",
    "from tinygrad.tensor import Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset\n",
    "\n",
    "The methodology in the paper prescribes a 65/15/20 train/val/test set with 5 cross validation splits. The data is found in `ticdata2000.txt` and the pair of `ticeval2000.txt` and `tictgts2000.txt` (tic=The Insurance Company, eval=Evaluation/Test, tgts=Targets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str, delimiter: str = \"\\t\", has_target: bool = True):\n",
    "    data = np.loadtxt(file_path, delimiter=delimiter)\n",
    "    if has_target:\n",
    "        X, y = data[:, :-1], data[:, -1]\n",
    "        return X, y\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_nonbinary_columns(X: np.ndarray) -> np.ndarray:\n",
    "    for i in range(X.shape[1]):\n",
    "        unique_vals = np.unique(X[:, i])\n",
    "        if not np.array_equal(unique_vals, [0, 1]):\n",
    "            X[:, i] = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/names.txt\", \"r\") as file:\n",
    "    feature_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "X_train, y_train = load_data(\"dataset/ticdata2000.txt\")\n",
    "X_train = normalize_nonbinary_columns(X_train)\n",
    "\n",
    "X_test = load_data(\"dataset/ticeval2000.txt\", has_target=False)\n",
    "X_test = normalize_nonbinary_columns(X_test)\n",
    "y_test = np.loadtxt(\"dataset/tictgts2000.txt\", delimiter=\"\\t\")\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.concatenate((y_train, y_test))\n",
    "\n",
    "num_samples = X_combined.shape[0]\n",
    "indices = np.random.permutation(num_samples)\n",
    "X_shuffled = X_combined[indices]\n",
    "y_shuffled = y_combined[indices]\n",
    "\n",
    "train_end = int(0.65 * num_samples)\n",
    "val_end = int((0.65 + 0.15) * num_samples)\n",
    "\n",
    "X_train_new = X_shuffled[:train_end]\n",
    "y_train_new = y_shuffled[:train_end]\n",
    "X_val_new = X_shuffled[train_end:val_end]\n",
    "y_val_new = y_shuffled[train_end:val_end]\n",
    "X_test_new = X_shuffled[val_end:]\n",
    "y_test_new = y_shuffled[val_end:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "- 2 hidden layers:\n",
    "  - $l$ is the input size\n",
    "  - 1<sup>st</sup> hidden layer had $m_1l$ units where $1\\le m_1\\le 8$.\n",
    "  - 2<sup>nd</sup> hidden layer had $m_2l$ units where $1\\le m_2\\le 3$.\n",
    "- SELU Activation Function.\n",
    "- Batch Normalization after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L507\n",
    "class Selu(Function):\n",
    "    _alpha: float = 1.6732632423543772848170429916717\n",
    "    _lambda: float = 1.0507009873554804934193349852946\n",
    "\n",
    "    def forward(self, x: LazyBuffer) -> LazyBuffer:\n",
    "        alpha_buf = x.const_like(self._alpha)\n",
    "        lambda_buf = x.const_like(self._lambda)\n",
    "        self.ret = lambda_buf * LazyBuffer.where(\n",
    "            x >= 0, x, alpha_buf * ((x * (1 / math.log(2))).exp2() - 1)\n",
    "        )\n",
    "        return self.ret\n",
    "\n",
    "    def backward(self, grad_output: LazyBuffer) -> LazyBuffer:\n",
    "        alpha_buf = self.ret.const_like(self._alpha)\n",
    "        lambda_buf = self.ret.const_like(self._lambda)\n",
    "        dx = LazyBuffer.where(\n",
    "            self.ret >= 0,\n",
    "            lambda_buf,\n",
    "            lambda_buf * alpha_buf * (self.ret * (1 / math.log(2))).exp2(),\n",
    "        )\n",
    "        return dx * grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, l: int, m1: int = 4, m2: int = 2) -> None:\n",
    "        self.layers: List[Callable[[Tensor], Tensor]] = [\n",
    "            nn.Linear(l, m1 * l),\n",
    "            nn.BatchNorm(m1 * l),\n",
    "            Selu.apply,\n",
    "            nn.Linear(m1 * l, m2 * l),\n",
    "            nn.BatchNorm(m2 * l),\n",
    "            Selu.apply,\n",
    "            nn.Linear(m2 * l, 1),\n",
    "            Tensor.sigmoid,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return x.sequential(self.layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "- Evaluation metric was Area under the Curve (AUC).\n",
    "- Cross Entropy Loss.\n",
    "- AdamW optimizer.\n",
    "- Constant Learning Rate (What value?).\n",
    "- Trained with early stopping based on the performance of validation set.\n",
    "  - Stopping patience (# of epochs) was 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:   0.00 test_accuracy:  0.47%: 100%|███████| 15/15 [00:09<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtest_acc=0.4680425426481789 >= 0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = MLP(l=X_train_new.shape[1])\n",
    "optim = nn.optim.AdamW(nn.state.get_parameters(model))\n",
    "\n",
    "@Tensor.train()\n",
    "def train_step() -> Tensor:\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    samples = np.random.randint(0, X_train_new.shape[0], 128)\n",
    "    X_batch = Tensor(X_train_new[samples], dtype=\"float32\")\n",
    "    y_batch = Tensor(y_train_new[samples], dtype=\"float32\")\n",
    "\n",
    "    loss = model(X_batch).cross_entropy(y_batch).backward()\n",
    "    optim.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "@Tensor.test()\n",
    "def get_test_auc() -> float:\n",
    "    y_pred = model(Tensor(X_test_new, dtype=\"float32\"))\n",
    "    \n",
    "    y_true = y_test_new.astype(\"float32\")\n",
    "    auc = roc_auc_score(y_true, y_pred.numpy())\n",
    "    \n",
    "    return auc\n",
    "\n",
    "\n",
    "test_acc = float(\"nan\")\n",
    "for i in (t := trange(15)):\n",
    "    loss = train_step()\n",
    "    test_acc = get_test_auc().item()\n",
    "    t.set_description(f\"loss: {loss.item():6.2f} test_accuracy: {test_acc:5.2f}%\")\n",
    "\n",
    "if 0.0 <= test_acc < 100:\n",
    "    print(colored(f\"{test_acc=} >= 0.0\", \"green\"))\n",
    "else:\n",
    "    raise ValueError(colored(f\"{test_acc=} < 0.0\", \"red\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
